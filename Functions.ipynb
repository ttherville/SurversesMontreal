{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c4e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV, Dates, XLSX, RollingFunctions, GLM, LinearAlgebra, MLBase, Combinatorics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8aab6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getRainData (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getRainData(OBXL::Int)\n",
    "Returns a dataframe with the rain data from the specific hyetogram\n",
    "\n",
    "# Arguments\n",
    "- `OBXL::Int`: The number corresponding to the hyetogram of interest.\n",
    "\"\"\"\n",
    "\n",
    "function getRainData(OBXL::Int)\n",
    "   return CSV.read(\"C:/Users/takum/Downloads/Recherche/TakumiTherville/Pluies/Hyétogramme_treated/OBXL\"*string(OBXL)*\"_treated\",DataFrame)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "482e06c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getCSOData (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getCSOData(site::String)\n",
    "Returns a dataframe with the CSO data from the specific site\n",
    "\n",
    "# Arguments\n",
    "- `site::String`: The name of the CSO site of interest.\n",
    "\"\"\"\n",
    "\n",
    "function getCSOData(site::String)\n",
    "    data_cso = CSV.read(\"C:/Users/takum/Downloads/Recherche/AlexandrineLanson/Données/cso_raw.csv\",DataFrame)\n",
    "    selectSite!(data_cso, \"Site\", site)\n",
    "    return data_cso\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586d15cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getCSOData (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getCSOData(site::Vector{String})\n",
    "Returns a dataframe with the CSO data from the specific sites\n",
    "\n",
    "# Arguments\n",
    "- `site::Vector{String}`: The names of the CSO sites of interest.\n",
    "\"\"\"\n",
    "\n",
    "function getCSOData(site::Vector{String})\n",
    "    data_cso = CSV.read(\"C:/Users/takum/Downloads/Recherche/AlexandrineLanson/Données/cso_raw.csv\",DataFrame)\n",
    "    selectSite!(data_cso, \"Site\", site)\n",
    "    return data_cso\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf543dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getData (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getData(OBXL::Int, site::String)\n",
    "Returns one dataframe with the Rain data and another with the CSO data.\n",
    "\n",
    "# Arguments\n",
    "- `OBXL::Int`: The number corresponding to the hyetogram of interest.\n",
    "- `site::String`: The name of the CSO site of interest.\n",
    "\"\"\"\n",
    "\n",
    "function getData(OBXL::Int, site::String)\n",
    "    return getRainData(OBXL), getCSOData(site) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b00f8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "removeCalibration! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    removeCalibration!(df::DataFrame, colValidity::String, calibrationTag::String)\n",
    "Removes all rows for which the hyetogram was in a calibration state.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colValidity::String`: The name of the column of `df` containing Validity data.\n",
    "- `calibrationTag::String` : The tag corresponding to the calibration state.\n",
    "\"\"\"\n",
    "\n",
    "function removeCalibration!(df::DataFrame, colValidity::String, calibrationTag::String)\n",
    "    filter!(colValidity => x -> ismissing(x) || !occursin(calibrationTag, x), df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516993c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fillDates_RainData (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    fillDates_RainData(df::DataFrame, colDate::String, colRain::String, from::DateTime, to::DateTime, by::Int = 5)\n",
    "Fills the dataframe with the missing Dates in a given period.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colDate::String`: The name of the column of `df` containing the dates.\n",
    "- `colRain::String`: The name of the column of `df` containing the rainfall data.\n",
    "- `from::DateTime` : The beginning date of the period of interest to be filled in.\n",
    "- `to::DateTime` : The end of the period of interest to be filled in.\n",
    "- `by::Int` : Time discretisation step in minutes, 5 minutes per default.\n",
    "\n",
    "#Implementation \n",
    "The function first creates all the missing dates in the period. \n",
    "Then sets the rainfall value for the new dates to 0.\n",
    "\"\"\"\n",
    "\n",
    "function fillDates_RainData(df::DataFrame, colDate::String, colRain::String, from::DateTime, to::DateTime; by::Int = 5)\n",
    "    dates = DataFrame(colDate => collect(from:Dates.Minute(by):to))\n",
    "    df_new = outerjoin(dates, df, on = Symbol(colDate))\n",
    "    df_new[ismissing.(df_new[!,colRain]),colRain] .= 0\n",
    "    \n",
    "    return df_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38678f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fillDates_RainData (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    fillDates_RainData(df::DataFrame, colDate::String, colRain::String, from::Int, to::Int; by::Int = 5)\n",
    "Fills the dataframe with the missing Dates between two years.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colDate::String`: The name of the column of `df` containing the dates.\n",
    "- `colRain::String`: The name of the column of `df` containing the rainfall data.\n",
    "- `from::DateTime` : The starting year of the dataframe to be returned.\n",
    "- `to::DateTime` : The ending year of the dataframe to be returned.\n",
    "- `by::Int` : Time discretisation step in minutes, 5 minutes per default.\n",
    "\n",
    "#Implementation \n",
    "The function first creates all the missing dates between the two years. \n",
    "Let us note that the end is not included in the period.\n",
    "Then sets the rainfall value for the new dates to 0.\n",
    "\"\"\"\n",
    "\n",
    "function fillDates_RainData(df::DataFrame, colDate::String, colRain::String, from::Int, to::Int; by::Int = 5)\n",
    "    dates = DataFrame(colDate => collect(DateTime(from,1,1,0,0,0):Dates.Minute(by):DateTime(to,1,1,0,0,0)))\n",
    "    df_new = outerjoin(dates, df, on = Symbol(colDate))\n",
    "    df_new[ismissing.(df_new[!,colRain]),colRain] .= 0\n",
    "    \n",
    "    return df_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72dbf0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createAccumulationData (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    createAccumulationData(df::DataFrame,colDay::String, colRain::String, varNames::Vector{Symbol}, maxNb::Vector{Int})\n",
    "Creates the maximum accumulations dataframe for the requested accumulation durations.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colDay::String`: The name of the column of `df` containing the days.\n",
    "- `colRain::String`: The name of the column of `df` containing the rainfall data.\n",
    "- `varNames::Vector{Symbol}` : The vector of names to be given to the new max accumulation columns.\n",
    "- `maxNb::Vector{Int}` : The number of times the initial discretisation step can be taken for each accumulation duration.\n",
    "\"\"\"\n",
    "\n",
    "function createAccumulationData(df::DataFrame,colDay::String, colRain::String, varNames::Vector{Symbol}, maxNb::Vector{Int})\n",
    "    \n",
    "    \n",
    "    \n",
    "    df[!,varNames[1]] = df[!,colRain]\n",
    "\n",
    "    for i in 2:length(varNames)\n",
    "        df[!, varNames[i]] = running(sum, df[!,colRain], maxNb[i])\n",
    "    end\n",
    "    \n",
    "    #Set the column order\n",
    "    df = df[!, vcat(Symbol(colDay), varNames)]\n",
    "    \n",
    "    #Split the data by Date\n",
    "    df_split = groupby(df, Symbol(colDay))\n",
    "\n",
    "    #Create the empty dataframe with the daily rain accumulations and add a row for each day with the maximum accumulation\n",
    "    df_daily = hcat(DataFrame(Symbol(colDay) => Date[]),DataFrame([name => Float64[] for name in vcat(varNames)]))\n",
    "\n",
    "    \n",
    "    for i in 1:length(df_split)\n",
    "        this_day_maxs = describe(df_split[i]).max\n",
    "        push!(df_daily, this_day_maxs)\n",
    "    end\n",
    "    \n",
    "    #Rename the day column to Date to allow for easy joining later with the cso dataframe\n",
    "    rename!(df_daily, Symbol(colDay) => :Date)\n",
    "    \n",
    "    return df_daily\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a185d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectSite! (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    selectSite!(df::DataFrame, colSite::String, site::String)\n",
    "Selects a certain CSO site data.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colSite::String`: The name of the column of `df` containing which CSO Site each data entry is from.\n",
    "- `site::String` : The site of interest.\n",
    "\"\"\"\n",
    "\n",
    "function selectSite!(df::DataFrame, colSite::String, site::String)\n",
    "    filter!(colSite => x -> x == site, df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f934a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    selectSite!(df::DataFrame, colSite::String, site::Vector{String})\n",
    "Selects certain CSO sites data.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colSite::String`: The name of the column of `df` containing which CSO Site each data entry is from.\n",
    "- `site::Vector{String}` : The sites of interest.\n",
    "\"\"\"\n",
    "\n",
    "function selectSite!(df::DataFrame, colSite::String, site::Vector{String})\n",
    "    filter!(colSite => x -> x in site, df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44e0383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectCause! (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    selectCause!(df::DataFrame, colCause::String, cause::String, acceptMissing::Bool)\n",
    "Selects a CSO occurence cause in the data.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colCause::String`: The name of the column of `df` containing the cause of each CSO.\n",
    "- `cause::String` : The cause of interest.\n",
    "- `acceptMissing::Bool` : Whether or not Missing entries of the cause should be kept in the data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "function selectCause!(df::DataFrame, colCause::String, cause::String, acceptMissing::Bool)\n",
    "    if acceptMissing \n",
    "        filter!(colCause => x -> ismissing(x) || x == cause, df)\n",
    "    else\n",
    "        filter!(colCause => x ->  !ismissing(x) && x == cause, df)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "464a8500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createDayCol! (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    createDayCol!(df::DataFrame, colDay::String, colDateTime::String)\n",
    "Creates a column corresponding to the day.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colDay::String`: The name to be given to the new day column.\n",
    "- `colDateTime::String` : The name of the column of `df` containing the date and time.\n",
    "\"\"\"\n",
    "\n",
    "function createDayCol!(df::DataFrame, colDay::String, colDateTime::String)\n",
    "    df[!,colDay] = [Date(Dates.year(i),Dates.month(i),Dates.day(i)) for i in df[!,colDateTime]]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bdb17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toMMRainfall! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    toMMRainfall!(df::DataFrame, colRain::String)\n",
    "Converts the rainfall data from mm/hour to mm*10^-1. \n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colRain::String`: The name of the column of `df` containing the rainfall data.\n",
    "\"\"\"\n",
    "\n",
    "function toMMRainfall!(df::DataFrame, colRain::String)\n",
    "   df[!,colRain] = df[!,colRain]*10/12 \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49ec0244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "createOverflowCol! (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    createOverflowCol!(df::DataFrame, colCSO::String, colDuration::String)\n",
    "Creates a boolean column indicating whether or not a CSO occured from the CSO durations.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colCSO::String`: The name to be given to the new CSO column.\n",
    "- `colDuration::String` : The name of the column of `df` containing the duration of the CSOs.\n",
    "\"\"\"\n",
    "\n",
    "function createOverflowCol!(df::DataFrame, colCSO::String, colDuration::String)\n",
    "    df[!,colCSO] .= (df[!,colDuration] .!= 0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8888306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deleteCSOAnomalies! (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    deleteCSOAnomalies!(df::DataFrame, col::String, colCSO::String, thresholdCol::Float64, CSOstate::bool)\n",
    "Delete rows in the dataframe for which a certain entry is lower than a threshold in a specific CSO state.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `col::String`: The name of the column of `df` containing the data of interest.\n",
    "- `colCSO::String` : The name of the column of `df` containing the CSO occurence.\n",
    "- `thrsholdCol::Float64` : The threshold under (including) which  the values of the data of interest should be deleted.\n",
    "- `CSOstate::bool` : The occurence (boolean) of CSO of interest.\n",
    "\"\"\"\n",
    "\n",
    "function deleteCSOAnomalies!(df::DataFrame, col::String, colCSO::String, thresholdCol::Float64, CSOstate::Bool)\n",
    "    filter!(row-> (row[colCSO] != CSOstate) || (row[col] >= thresholdCol && row[colCSO] == CSOstate), df)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb39dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preTreatRainData (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    preTreatRainData(df::DataFrame)\n",
    "Returns the pretreated rainfall data.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the rainfall data.\n",
    "\"\"\"\n",
    "\n",
    "function preTreatRainData(df::DataFrame)\n",
    "    \n",
    "    removeCalibration!(df, \"Validity\", \"CALIBRATION\")\n",
    "    \n",
    "    df_full = fillDates_RainData(df, \"Date\", \"Rainfall\", 2013, 2022)\n",
    "\n",
    "    toMMRainfall!(df_full, \"Rainfall\")\n",
    "\n",
    "    select!(df_full, Not([:Validity]))\n",
    "    \n",
    "    createDayCol!(df_full, \"Day\", \"Date\")\n",
    "    \n",
    "    df_full = sort(df_full, :Date)\n",
    "    \n",
    "    varnames = [:d5min, :d10min, :d15min, :d20min, :d25min,\n",
    "            :d30min, :d35min, :d40min, :d45min, :d50min,\n",
    "            :d55min, :d1h, :d2h, :d3h, :d4h,\n",
    "            :d6h, :d8h, :d12h, :d18h, :d24h]\n",
    "\n",
    "    maxNb = [1, 2, 3, 4, 5,\n",
    "        6, 7, 8, 9, 10,\n",
    "        11, 12, 24, 36, 48,\n",
    "        72, 96, 144, 216, 288]\n",
    "\n",
    "    df_daily = createAccumulationData(df_full,\"Day\",\"Rainfall\",varnames,maxNb)\n",
    "    \n",
    "    return df_daily\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e9f516d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[2]:11",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    preTreatRainData(df::DataFrame, varnames::Vector{Symbol}, maxNb::Vector{Int})\n",
    "Returns the pretreated rainfall data with specified accumulation variables.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the rainfall data.\n",
    "- `varnames::Vector{Symbol}`: The names of the accumulation data to be computed.\n",
    "- `maxNb::Vector{Int}`: The number of periods corresponding to the accumulation.\n",
    "- `by::Int`: The time discretization step of the data in minutes.\n",
    "\"\"\"\n",
    "\n",
    "function preTreatRainData(df::DataFrame, varnames::Vector{Symbol}, maxNb::Vector{Int})\n",
    "    \n",
    "    removeCalibration!(df, \"Validity\", \"CALIBRATION\")\n",
    "    \n",
    "    df_full = df\n",
    "\n",
    "    select!(df_full, Not([:Validity]))\n",
    "    \n",
    "    createDayCol!(df_full, \"Day\", \"Date\")\n",
    "    \n",
    "    df_full = sort(df_full, :Date)\n",
    "\n",
    "    df_daily = createAccumulationData(df_full,\"Day\",\"Rainfall\",varnames,maxNb)\n",
    "    \n",
    "    return df_daily\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e71974e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preTreatFullData! (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    preTreatFullData!(df::DataFrame)\n",
    "Pretreats the full data resulting of the joined rainfall and CSO data.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the full data.\n",
    "\"\"\"\n",
    "\n",
    "function preTreatFullData!(df::DataFrame)\n",
    "    select!(df, Not([:Monitored, :Comment, :Site, :Code]))\n",
    "\n",
    "    dropmissing!(df, :Duration)\n",
    "    dropmissing!(df, :d5min)\n",
    "\n",
    "    createOverflowCol!(df, \"CSO\", \"Duration\")\n",
    "\n",
    "    #deleteCSOAnomalies!(df, \"d24h\", \"CSO\", 5.0, true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefcb054",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:13",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    preTreatFullData!(df::DataFrame, colDuration::String, cold5::String, cold24::String, colToDel::Vector{String})\n",
    "Pretreats the full data resulting of the joined rainfall and CSO data.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the full data.\n",
    "- `colDuration::String`: The name of the column of `df` containing the durations.\n",
    "- `cold5::String`: The name of the column of `df` containing the minimal accumulation data.\n",
    "- `cold24::String`: The name of the column of `df` containing the maximal accumulation data.\n",
    "- `colToDel::Vector{String}`: The names of the column of `df` to be deleted.\n",
    "\"\"\"\n",
    "\n",
    "function preTreatFullData!(df::DataFrame, colDuration::String, cold5::String, cold24::String, colToDel::Vector{String})\n",
    "    select!(df, Not(Symbol.(colToDel)))\n",
    "\n",
    "    dropmissing!(df, Symbol(colDuration))\n",
    "    dropmissing!(df, Symbol(cold5))\n",
    "\n",
    "    createOverflowCol!(df, \"CSO\", colDuration)\n",
    "\n",
    "    #deleteCSOAnomalies!(df, cold24, \"CSO\", 5.0, true)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c142234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "splitDataset (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    splitDataset(df::DataFrame, colDay::String ,year::Int)\n",
    "Split the dataframe into a training set and a test set.\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: The dataframe containing the data.\n",
    "- `colDay::String`: The name of the column of `df` containing the dates.\n",
    "- `year::Int` : The year at which the data should be split.\n",
    "\n",
    "#Implementation\n",
    "The data is split such that the test data starts with the specified year.\n",
    "\"\"\"\n",
    "\n",
    "function splitDataset(df::DataFrame, colDay::String ,year::Int)\n",
    "    train = df[df[!,colDay] .< Date(year,1,1),:]\n",
    "    test = df[df[!,colDay] .>= Date(year,1,1),:]\n",
    "    \n",
    "    return train, test\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737eebe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1 (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    F1(predictions, trueValues)\n",
    "Computes the F1 score given the predictions and the true values.\n",
    "\n",
    "# Arguments\n",
    "- `predictions`: The predicted valuesin a 1D table form.\n",
    "- `trueValues`: The true values in a 1D table form.\n",
    "\"\"\"\n",
    "\n",
    "function F1(predictions, trueValues)\n",
    "    \n",
    "    tp = 0\n",
    "    for i in 1:length(trueValues)\n",
    "        if trueValues[i] == 1 && predictions[i] == 1\n",
    "            tp = tp + 1\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return 2*tp/(2*tp + sum(predictions.!= trueValues))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e2591b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_F1 (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    compute_F1(M, test)\n",
    "Computes the maximum F1 score using different thresholds. \n",
    "\n",
    "# Arguments\n",
    "- `M`: The predicted model in a 1D table form.\n",
    "- `test::DataFrame`: The true values in a 1D table form.\n",
    "- `colToPredict::String` : The name of the column of `test` containing the variable of interest's data.\n",
    "\"\"\"\n",
    "\n",
    "function compute_F1(M, test::DataFrame, colToPredict::String)\n",
    "    \n",
    "    predictedvals = [predict(M,test)[i] .> j for i in 1:size(test)[1], j in 0.1:0.01:0.6]\n",
    "    groundtruthvals = test[!,Symbol.(colToPredict)]\n",
    "    \n",
    "    F1_scores = [F1(predictedvals[:,j], groundtruthvals) for j in 1:size(predictedvals,2)]\n",
    "    #print(\"(index, the corresponding best threshold)\")\n",
    "    show((argmax(F1_scores), collect(0.1:0.01:0.6)[argmax(F1_scores)]))\n",
    "    \n",
    "    return maximum(F1_scores), collect(0.1:0.01:0.6)[argmax(F1_scores)]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cb061b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[2]:11"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    compute_best_F1_threshold(M, test)\n",
    "Computes the threshold leading to the maximum F1 score.\n",
    "\n",
    "# Arguments\n",
    "- `M`: The predicted model in a 1D table form.\n",
    "- `test::DataFrame`: The true values in a 1D table form.\n",
    "- `colToPredict::String` : The name of the column of `test` containing the variable of interest's data.\n",
    "\"\"\"\n",
    "\n",
    "function compute_best_F1_threshold(M, test::DataFrame, colToPredict::String)\n",
    "    \n",
    "    predictedvals = [predict(M,test)[i] .> j for i in 1:size(test)[1], j in 0.1:0.01:0.6]\n",
    "    groundtruthvals = test[!,colToPredict]\n",
    "    \n",
    "    F1_scores = [F1(predictedvals[:,j], groundtruthvals) for j in 1:size(predictedvals,2)]\n",
    "    #print(\"(index, the corresponding best threshold)\")\n",
    "    show((argmax(F1_scores), collect(0.1:0.01:0.6)[argmax(F1_scores)]))\n",
    "    \n",
    "    return collect(0.1:0.01:0.6)[argmax(F1_scores)]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f1ec2f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[5]:11",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    compute_MSE(M, test)\n",
    "Computes the Mean Squared Error.\n",
    "\n",
    "# Arguments\n",
    "- `M`: The predicted model in a 1D table form.\n",
    "- `test::DataFrame`: The true values in a 1D table form.\n",
    "- `colToPredict::String` : The name of the column of `df` containing the variable of interest's data.\n",
    "\"\"\"\n",
    "\n",
    "function compute_MSE(M, test::DataFrame, colToPredict::String)\n",
    "    \n",
    "    predictedvals = predict(M,test)\n",
    "    groundtruthvals = test[!,colToPredict]\n",
    "    \n",
    "    MSE = sum((predictedvals-groundtruthvals).^2)/(length(predictedvals))\n",
    "    \n",
    "    return MSE\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5be2d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[4]:11",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    compute_RMSD(M, test)\n",
    "Computes the Root mean squared deviation.\n",
    "\n",
    "# Arguments\n",
    "- `M`: The predicted model in a 1D table form.\n",
    "- `test::DataFrame`: The true values in a 1D table form.\n",
    "- `colToPredict::String` : The name of the column of `df` containing the variable of interest's data.\n",
    "\"\"\"\n",
    "\n",
    "function compute_RMSD(M, test::DataFrame, colToPredict::String)\n",
    "    \n",
    "    return sqrt(compute_MSE(M, test, colToPredict))\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f1e7be",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[5]:11"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getLogisticModel(train::DataFrame, toPredict::String, predictors::Vector{String})\n",
    "Returns the logistic regression model with the given predictors and variable of interest.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "\"\"\"\n",
    "\n",
    "function getLogisticModel(train::DataFrame, toPredict::String, predictors::Vector{String})\n",
    "    \n",
    "    \n",
    "    formula = Term(Symbol(toPredict)) ~ sum(term.(predictors))\n",
    "    local m\n",
    "    try \n",
    "        m = glm(formula, train, Bernoulli(), LogitLink(), maxiter = 100)\n",
    "    catch e\n",
    "        return 0.0\n",
    "    else\n",
    "        return m\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6395eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    getNormalModel(train::DataFrame, toPredict::String, predictors::Vector{String}, link::String)\n",
    "Returns the normal regression model with the given predictors and variable of interest.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `link::String` : The link function to be used.\n",
    "\"\"\"\n",
    "\n",
    "function getNormalModel(train::DataFrame, toPredict::String, predictors::Vector{String}, link::String)\n",
    "    \n",
    "    formula = Term(Symbol(toPredict)) ~ sum(term.(predictors))\n",
    "    if link == \"inverse\"\n",
    "        m = glm(formula, train, Normal(), InverseLink(), maxiter = 100)\n",
    "    elseif link == \"log\"\n",
    "        m = glm(formula, train, Normal(), LogLink(), maxiter = 100)\n",
    "    elseif link == \"identity\"\n",
    "        m = glm(formula, train, Normal(), maxiter = 100)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    return m\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98d2ac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[3]:11"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getGammaModel(train::DataFrame, toPredict::String, predictors::Vector{String})\n",
    "Returns the gamma regression model with the given predictors and variable of interest.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "\"\"\"\n",
    "\n",
    "function getGammaModel(train::DataFrame, toPredict::String, predictors::Vector{String},link::String)\n",
    "    \n",
    "    formula = Term(Symbol(toPredict)) ~ sum(term.(predictors))\n",
    "    if link == \"inverse\"\n",
    "        m = glm(formula, train, Gamma(), InverseLink(), maxiter = 1000)\n",
    "    elseif link == \"log\"\n",
    "        m = glm(formula, train, Gamma(), LogLink(), maxiter = 1000)\n",
    "    end\n",
    "    \n",
    "    return m\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d594b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    getLinearModel(train::DataFrame, toPredict::String, predictors::Vector{String})\n",
    "Returns the linear regression model with the given predictors and variable of interest.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The dataframe containing the training data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "\"\"\"\n",
    "\n",
    "function getLinearModel(train::DataFrame, toPredict::String, predictors::Vector{String})\n",
    "    \n",
    "    formula = Term(Symbol(toPredict)) ~ sum(term.(predictors))\n",
    "    m = lm(formula, train)\n",
    "    \n",
    "    return m\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "053d41ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[8]:12",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getTreeModel(train::DataFrame, toPredict::String, predictors::Vector{String}, max_depth::Int)\n",
    "Returns the regression tree model with the given predictors and variable of interest.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The dataframe containing the training data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `max_depth::Int` : The maximum tree depth.\n",
    "\"\"\"\n",
    "\n",
    "function getTreeModel(train::DataFrame, toPredict::String, predictors::Vector{String}, max_depth::Int)\n",
    "    \n",
    "    modelDT = DecisionTreeClassifier(max_depth = max_depth)\n",
    "    tree = DecisionTree.fit!(modelDT, Matrix(train[:, predictors]), train[:,toPredict])\n",
    "\n",
    "    return tree\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bf461b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[20]:17",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchTree(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, max_depth::Int)\n",
    "Returns the F1 score of all possible logistic regression models in decreasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "\n",
    "#Implementation\n",
    "The returned F1 score corresponds to the best possible F1 score with different prediction thresholds in the range of 0.1 \n",
    "to 0.6 with a 0.01 increment. \n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchTree(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, max_depth::Int)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "\n",
    "        model = getTreeModel(train, toPredict, i, max_depth)\n",
    "        pred = DecisionTree.predict(model, Matrix(test[:, i]))\n",
    "        \n",
    "        \n",
    "        s = rmsd(pred, test[:, toPredict])\n",
    "        scores[l] = s\n",
    "        l = l+1\n",
    "        \n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    return sort(DataFrame(Variables = collect(combinations), RMSD = scores), :RMSD)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f682850",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[8]:18"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchLogistic(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String})\n",
    "Returns the F1 score of all possible logistic regression models in decreasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `threshold::Float64` : The value of the classification threshold. A value of 0 indicates that the best classification threshold should be computed and used \n",
    "\n",
    "#Implementation\n",
    "The returned F1 score corresponds to the best possible F1 score with different prediction thresholds in the range of 0.1 \n",
    "to 0.6 with a 0.01 increment. \n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLogistic(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, threshold::Float64)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    f_scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    if threshold == 0\n",
    "        thresholds = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    end\n",
    "    \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        \n",
    "        model = getLogisticModel(train, toPredict, i)\n",
    "        \n",
    "        if typeof(model) == Float64\n",
    "            println(\"Error in model fitting\")\n",
    "            \n",
    "            f = -Inf \n",
    "            if threshold == 0.0\n",
    "                thresholds[l] = 1.0\n",
    "            end\n",
    "        else\n",
    "            if threshold != 0.0\n",
    "                f = F1([predict(model,test)[i] .> threshold for i in 1:size(test)[1]], test[!,toPredict])\n",
    "            else\n",
    "            \n",
    "                function fobj(θ::Real)\n",
    "                    scores = convert(Vector{Float64}, MLBase.predict(model, train))\n",
    "                    r = roc(train.CSO, scores, θ)\n",
    "                    return -f1score(r)\n",
    "                end\n",
    "            \n",
    "                res = optimize(fobj, 0.0, 1.0)\n",
    "                thresholds[l] = res.minimizer\n",
    "            \n",
    "                f = - res.minimum\n",
    "            \n",
    "            end\n",
    "        end\n",
    "        \n",
    "\n",
    "        f_scores[l] = f\n",
    "        l = l+1\n",
    "        #print(\" The corresponding F score is \")\n",
    "        #println(f)\n",
    "    end\n",
    "    \n",
    "    if threshold == 0\n",
    "        return sort(DataFrame(Variables = collect(combinations), F1 = f_scores, Threshold = thresholds), :F1, rev = true)\n",
    "    else\n",
    "        return sort(DataFrame(Variables = collect(combinations), F1 = f_scores), :F1, rev = true)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2113ee97",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[4]:16",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchLinear(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String)\n",
    "Returns the mean squared error of all possible linear regression models in increasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `criteria::String` : The goodness of fit indicator to be used.\n",
    "\n",
    "#Implementation\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLinear(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    \n",
    "    if (criteria == \"RMSD_CSO\" )|| (criteria == \"MSE_CSO\")\n",
    "        l = 1\n",
    "        for i in combinations\n",
    "            model = getLinearModel(train[train[!,toPredict] .> 0,:], toPredict, i)\n",
    "            mse = compute_MSE(model, test[test[!,toPredict] .> 0, :], toPredict)\n",
    "            scores[l] = mse\n",
    "            l = l+1\n",
    "            #print(\" The corresponding MSE is \")\n",
    "            #println(f)\n",
    "        end\n",
    "        \n",
    "        if criteria == \"MSE\"\n",
    "            return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "        elseif criteria == \"RMSD\"\n",
    "            return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        model = getLinearModel(train, toPredict, i)\n",
    "        mse = compute_MSE(model, test, toPredict)\n",
    "        scores[l] = mse\n",
    "        l = l+1\n",
    "        #print(\" The corresponding MSE is \")\n",
    "        #println(f)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c8c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchNormal(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String, link::String)\n",
    "Returns the mean squared error of all possible gaussian regression models in increasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `criteria::String` : The goodness of fit indicator to be used.\n",
    "- `link::String` : The link function to be used.\n",
    "\n",
    "#Implementation\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchNormal(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String, link::String)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "        \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        model = getNormalModel(train, toPredict, i, link)\n",
    "\n",
    "        mse = compute_MSE(model, test, toPredict)\n",
    "        scores[l] = mse\n",
    "        l = l+1\n",
    "        #print(\" The corresponding MSE is \")\n",
    "        #println(f)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50829713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchGamma(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String, link::String)\n",
    "Returns the mean squared error or rmsd of all possible Gamma regression models in increasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `criteria::String` : The goodness of fit indicator to be used.\n",
    "- `link::String` : The link function to be used.\n",
    "\n",
    "#Implementation\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchGamma(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String, link::String)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "        \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        model = getGammaModel(train, toPredict, i, link)\n",
    "\n",
    "        mse = compute_MSE(model, test, toPredict)\n",
    "        scores[l] = mse\n",
    "        l = l+1\n",
    "        #print(\" The corresponding MSE is \")\n",
    "        #println(f)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fdd939b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getBestModelSiteOBXL (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getBestModelSiteOBXL(site::String, OBXL::Int, year::Int, Vars::Vector{String})\n",
    "Returns the best models based on the F1 score.\n",
    "\n",
    "# Arguments\n",
    "- `site::String`: The name of the CSO site of interest\n",
    "- `OBXL::Int`: The number corresponding to the hyetogram to be used.\n",
    "- `year::Int` : The year when the test data starts.\n",
    "- `Vars::Vector{String}`: The names of the variables to be used in the models.\n",
    "\n",
    "#Implementation\n",
    "The variables in Vars are supposed to be one of the next [:d5min, :d10min, :d15min, :d20min,\n",
    ":d25min, :d30min, :d35min, :d40min, :d45min, :d50min, :d55min, :d1h, :d2h, :d3h, :d4h,\n",
    ":d6h, :d8h, :d12h, :d18h, :d24h].\n",
    "\"\"\"\n",
    "\n",
    "function getBestModelSiteOBXL(site::String, OBXL::Int, year::Int, Vars::Vector{String})\n",
    "    \n",
    "    println(\"Importing data for OBXL\"*string(OBXL)*\" and site \"*site)\n",
    "    \n",
    "    data_rain, data_cso = getData(OBXL,site)\n",
    "    \n",
    "    data_rain_daily = preTreatRainData(data_rain)\n",
    "\n",
    "    selectCause!(data_cso, \"Code\", \"P\", true)\n",
    "    \n",
    "    data = outerjoin(data_rain_daily, data_cso, on = :Date) \n",
    "    \n",
    "    preTreatFullData!(data)\n",
    "    \n",
    "    train, test = splitDataset(data, \"Date\", 2020)\n",
    "    \n",
    "    f_scores = extensiveSearchLogistic(train, test, \"CSO\", Vars)\n",
    "    \n",
    "    sort(f_scores, :F1, rev=true)\n",
    "    \n",
    "    return groupby(f_scores, :F1)[1]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4a55bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bestModelFormatting (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    bestModelFormatting(sdf::SubDataFrame)\n",
    "Formats the input SubDataFrame containing the best model information into a tuple with the score and the best models.\n",
    "\n",
    "# Arguments\n",
    "- `sdf::SubDataFrame`: the sub dataframe containing the best models with their score. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "function bestModelFormatting(sdf::SubDataFrame)\n",
    "    return (sdf.F1[1], sdf.Variables)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4969cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getBestModelSitesOBXLs (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getBestModelSitesOBXLs(site::String, OBXL::Int, year::Int, Vars::Vector{String})\n",
    "Returns the best models for each Site and Hyetogram based on the F1 score.\n",
    "\n",
    "# Arguments\n",
    "- `sites::Vector{String}`: The names of the CSO sites of interest\n",
    "- `OBXLs::Vector{Int}`: The numbers corresponding to the hyetograms to be used.\n",
    "- `year::Int` : The year when the test data starts.\n",
    "- `Vars::Vector{String}`: The names of the variables to be used in the models.\n",
    "\n",
    "#Implementation\n",
    "The variables in Vars are supposed to be one of the next [:d5min, :d10min, :d15min, :d20min,\n",
    ":d25min, :d30min, :d35min, :d40min, :d45min, :d50min, :d55min, :d1h, :d2h, :d3h, :d4h,\n",
    ":d6h, :d8h, :d12h, :d18h, :d24h].\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "function getBestModelSitesOBXLs(sites::Vector{String}, OBXLs::Vector{Int}, year::Int, Vars::Vector{String})\n",
    "    n = length(OBXLs)\n",
    "    m = length(sites)\n",
    "    \n",
    "    out = [bestModelFormatting(getBestModelSiteOBXL(sites[i], OBXLs[j], year, Vars)) for i in 1:m, j in 1:n]\n",
    "    out = hcat(DataFrame(Site = sites), DataFrame(out, string.(OBXLs)))\n",
    "    \n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d625768f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getBestModelSitesOBXLs2 (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    getBestModelSitesOBXLs2(site::String, OBXL::Int, year::Int, Vars::Vector{String})\n",
    "Returns the best models for each Site and Hyetogram based on the F1 score.\n",
    "# Arguments\n",
    "- `sites::Vector{String}`: The names of the CSO sites of interest\n",
    "- `OBXLs::Vector{Int}`: The numbers corresponding to the hyetograms to be used.\n",
    "- `year::Int` : The year when the test data starts.\n",
    "- `Vars::Vector{String}`: The names of the variables to be used in the models.\n",
    "\n",
    "#Implementation\n",
    "The variables in Vars are supposed to be one of the next [:d5min, :d10min, :d15min, :d20min,\n",
    ":d25min, :d30min, :d35min, :d40min, :d45min, :d50min, :d55min, :d1h, :d2h, :d3h, :d4h,\n",
    ":d6h, :d8h, :d12h, :d18h, :d24h].\n",
    "\n",
    "The function getBestModelSiteOBXL(site::String, OBXL::Int, year::Int, Vars::Vector{String}) was not used as to minimize\n",
    "the run time. Let us note that this function should be later modified as to modify the outer for loop depending on the \n",
    "size of its arguments. Furthermore, the function body should be simplified into a simple function call which would take \n",
    "either a single site/OBXl and a vector of OBXLs/sites and compute the desired best models and F1 scores.Those \n",
    "implementations will be made later on.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "function getBestModelSitesOBXLs2(sites::Vector{String}, OBXLs::Vector{Int}, year::Int, Vars::Vector{String})\n",
    "    n = length(OBXLs)\n",
    "    m = length(sites)\n",
    "    out = Array{Any,2}(undef, m, n)\n",
    "    \n",
    "    for i in 1:m\n",
    "        data_cso = getCSOData(sites[i])\n",
    "        \n",
    "        for j in 1:n\n",
    "            println(\"Importing data for OBXL\"*string(OBXLs[j])*\" and site \"*sites[i])\n",
    "            \n",
    "            data_rain = getRainData(OBXLs[j])\n",
    "            \n",
    "            data_rain_daily = preTreatRainData(data_rain)\n",
    "\n",
    "            selectCause!(data_cso, \"Code\", \"P\", true)\n",
    "    \n",
    "            data = outerjoin(data_rain_daily, data_cso, on = :Date) \n",
    "        \n",
    "            preTreatFullData!(data)\n",
    "    \n",
    "            train, test = splitDataset(data, \"Date\", 2020)\n",
    "    \n",
    "            f_scores = extensiveSearchLogistic(train, test, \"CSO\", Vars)\n",
    "            \n",
    "            out[i,j] = bestModelFormatting(groupby(f_scores, :F1)[1])\n",
    "            \n",
    "            print(\"Best Models for \"*sites[i]*\" OBXL\"*string(OBXLs[j])*\" \")\n",
    "            println(out[i,j])\n",
    "            \n",
    "        end\n",
    "        \n",
    "        println(\"Best Models for \"*sites[i])\n",
    "        println(out[i,:])\n",
    "        \n",
    "        CSV.write(sites[i]*\"_bestModels.csv\", out[i,:])\n",
    "    end\n",
    "    \n",
    "    out = hcat(DataFrame(Site = sites), DataFrame(out, string.(OBXLs)))\n",
    "    \n",
    "    return out\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af8c54b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[6]:19",
      " [2] eval",
      "   @ .\\boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1116"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    forwardStepSearch(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, strictly::Bool)\n",
    "Returns the F1 score and the corresponding model of a forward model search.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The dataframe containing the training data for the model.\n",
    "- `test::DataFrame`: The dataframe containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `varNames::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `strictly::Bool` : A boolean indicating whether the search should be strictly increasing or not.\n",
    "\n",
    "#Implementation\n",
    "The search uses the F1 score as a criteria. \n",
    "The returned F1 score corresponds to the best possible F1 score with different prediction thresholds in the range of 0.1 \n",
    "to 0.6 with a 0.01 increment. \n",
    "The data is returned in the form of a 2 element tuple with first the F1 score and then the corresponding model.\n",
    "\"\"\"\n",
    "\n",
    "function forwardStepSearch(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, strictly::Bool)\n",
    "    \n",
    "    selectedVars = String[]\n",
    "    unselectedVars = varNames\n",
    "    \n",
    "    bestF1 = 0\n",
    "    \n",
    "    improved = true\n",
    "    \n",
    "    if strictly\n",
    "    \n",
    "        while improved \n",
    "        \n",
    "            thisBestF1 = bestF1\n",
    "            improved = false\n",
    "            thisBestPred = 0\n",
    "        \n",
    "            for newVar in unselectedVars  \n",
    "            \n",
    "                thisF1 = compute_F1(getLogisticModel(train, toPredict, vcat(selectedVars,newVar)), test, toPredict)\n",
    "                println(newVar, thisF1)\n",
    "            \n",
    "                if thisF1 > thisBestF1\n",
    "                    thisBestF1 = thisF1\n",
    "                    thisBestPred = newVar\n",
    "                    improved = true\n",
    "                end\n",
    "            end\n",
    "        \n",
    "        \n",
    "            if improved \n",
    "                println(\"The variable \"*thisBestPred*\" has been added with F1 \"*string(thisBestF1))\n",
    "                selectedVars = vcat(selectedVars,thisBestPred)\n",
    "                deleteat!(unselectedVars, findall(x->x== thisBestPred,unselectedVars))\n",
    "                bestF1 = thisBestF1\n",
    "            end\n",
    "        \n",
    "        \n",
    "        end\n",
    "        \n",
    "    else\n",
    "         selectedVarsString = String[]\n",
    "        \n",
    "         while improved \n",
    "        \n",
    "            thisBestF1 = bestF1\n",
    "            improved = false\n",
    "            thisBestPred = 0\n",
    "            equal = false\n",
    "        \n",
    "            for newVar in unselectedVars  \n",
    "            \n",
    "                thisF1 = compute_F1(getLogisticModel(train, toPredict, vcat(selectedVars,newVar)), test, toPredict)\n",
    "                println(newVar, thisF1)\n",
    "            \n",
    "                if (thisF1 > thisBestF1)\n",
    "                    thisBestF1 = thisF1\n",
    "                    thisBestPred = newVar\n",
    "                    improved = true\n",
    "                    equal = false\n",
    "                    \n",
    "                elseif (thisF1 == thisBestF1) && (improved == false)\n",
    "                    equal = true\n",
    "                    thisBestF1 = thisF1\n",
    "                    thisBestPred = newVar\n",
    "                    improved = true\n",
    "                end\n",
    "            end\n",
    "        \n",
    "        \n",
    "            if improved && equal\n",
    "                println(\"The variable \"*thisBestPred*\" has been added with same F1 \"*string(thisBestF1))\n",
    "                selectedVars = vcat(selectedVars,thisBestPred)\n",
    "                selectedVarsString = vcat(selectedVarsString,thisBestPred*\"s\")\n",
    "                deleteat!(unselectedVars, findall(x->x== thisBestPred,unselectedVars))\n",
    "                bestF1 = thisBestF1\n",
    "            elseif improved \n",
    "                println(\"The variable \"*thisBestPred*\" has been added with F1 \"*string(thisBestF1))\n",
    "                selectedVars = vcat(selectedVars,thisBestPred)\n",
    "                selectedVarsString = vcat(selectedVarsString,thisBestPred)\n",
    "                deleteat!(unselectedVars, findall(x->x== thisBestPred,unselectedVars))\n",
    "                bestF1 = thisBestF1\n",
    "            end\n",
    "        \n",
    "        \n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return (bestF1, selectedVarsString)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de2aa92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forwardStepSearch (generic function with 3 methods)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    forwardStepSearch(site::String, OBXL::Int, year::Int, Vars::Vector{String}, strictly::Bool)\n",
    "Returns the F1 score and the corresponding model of a forward model search.\n",
    "# Arguments\n",
    "- `sites::Vector{String}`: The names of the CSO sites of interest\n",
    "- `OBXLs::Vector{Int}`: The numbers corresponding to the hyetograms to be used.\n",
    "- `year::Int` : The year when the test data starts.\n",
    "- `Vars::Vector{String}`: The names of the variables to be used in the models.\n",
    "- `strictly::Bool` : A boolean indicating whether the search should be strictly increasing or not.\n",
    "\n",
    "#Implementation\n",
    "The search uses the F1 score as a criteria. \n",
    "The returned F1 score corresponds to the best possible F1 score with different prediction thresholds in the range of 0.1 \n",
    "to 0.6 with a 0.01 increment. \n",
    "The data is returned in the form of a 2 element tuple with first the F1 score and then the corresponding model.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "function forwardStepSearch(site::String, OBXL::Int, year::Int, Vars::Vector{String}, strictly::Bool)\n",
    "    \n",
    "    println(\"Importing data for OBXL\"*string(OBXL)*\" and site \"*site)\n",
    "    \n",
    "    data_rain, data_cso = getData(OBXL,site)\n",
    "    \n",
    "    data_rain_daily = preTreatRainData(data_rain)\n",
    "\n",
    "    selectCause!(data_cso, \"Code\", \"P\", true)\n",
    "    \n",
    "    data = outerjoin(data_rain_daily, data_cso, on = :Date) \n",
    "    \n",
    "    preTreatFullData!(data)\n",
    "    \n",
    "    train, test = splitDataset(data, \"Date\", 2020)\n",
    "    \n",
    "    f_score = forwardStepSearch(train, test, \"CSO\", Vars, strictly)\n",
    "    \n",
    "    return f_score\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6af3ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:18"
     ]
    }
   ],
   "source": [
    "using Combinatorics, Optim\n",
    "\"\"\"\n",
    "    extensiveSearchLogistic(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, threshold::Float64)\n",
    "Returns the F1 score of all possible logistic regression models in decreasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `threshold::Float64` : The value of the classification threshold. A value of 0 indicates that the best classification threshold should be computed and used \n",
    "\n",
    "#Implementation\n",
    "The returned F1 score corresponds to the F1 score of the logistic model using the best classification threshold on the training data. \n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLogistic(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, thresholds::Vector{Float64})\n",
    "    \n",
    "    println(thresholds)\n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    f_scores = Array{Float64}(undef, 2^length(varNames)-1, length(thresholds))\n",
    "    \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        \n",
    "        println(l)\n",
    "        \n",
    "        model = getLogisticModel(train, toPredict, i)\n",
    "        \n",
    "        scores = convert(Vector{Float64}, MLBase.predict(model, test))\n",
    "\n",
    "        f_scores[l,:] =  [f1score(roc(test[!,toPredict], scores, threshold)) for threshold in thresholds]\n",
    "        l = l+1\n",
    "        \n",
    "    end\n",
    "    \n",
    "    \n",
    "    d = DataFrame(hcat(collect(combinations),f_scores), :auto)\n",
    "    rename!(d, vcat(:Variables, Symbol.(string.(thresholds))))\n",
    "    \n",
    "    return d\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "count_flags(s::String, flag::String)\n",
    "\n",
    "counts the number of flags `flag` in string `s`.\n",
    "\"\"\"\n",
    "function count_flags(s::String, flag::String)\n",
    "counter = 0\n",
    "for i in 1:length(s)\n",
    "  if occursin(flag, s)\n",
    "    s = replace(s, flag=> \"\", count=1)\n",
    "    counter+=1\n",
    "  else\n",
    "    break\n",
    "  end\n",
    "end\n",
    "return counter\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540a5b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataTables\n",
    "\n",
    "\"\"\"\n",
    "    extensiveSearchLogisticRUS(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, n::Int)\n",
    "Returns the F1 score of all possible logistic regression models in decreasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `n::Int` : The number of Randomly Undersampled datasets to use\n",
    "\n",
    "#Implementation\n",
    "The mean F1 score is computed from the individual F1 score on the test data of the models trained on \n",
    "100 randomly undersampled datasets.\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on average on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLogisticRUS(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, n::Int)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    f_scores = Array{Float64}(undef, 2^length(varNames)-1, n)\n",
    "    \n",
    "    first = DataFrame(DataTable(undersample(row->row[Symbol(toPredict)], DataTable(train))))\n",
    "    l_first = size(first)[1]\n",
    "    rus = hcat(first, DataFrame(RUS = Int.(ones(l_first))))\n",
    "    \n",
    "    for i in 2:n\n",
    "        rus = vcat(rus, hcat(DataFrame(DataTable(undersample(row->row[Symbol(toPredict)], DataTable(train)))), DataFrame(RUS = Int.(i .* ones(l_first)))))\n",
    "    end\n",
    "    \n",
    "    rus_j = groupby(rus, :RUS)\n",
    "    \n",
    "    i = 1\n",
    "    for k in combinations\n",
    "        \n",
    "        \n",
    "        for j in rus_j\n",
    "            \n",
    "            local model\n",
    "            \n",
    "            \n",
    "            model = getLogisticModel(DataFrame(j), toPredict, k)\n",
    "            if model == 0.0\n",
    "                println(\"Error in model fitting\")\n",
    "                f = 0\n",
    "            else \n",
    "                f = f1score(roc(test[!,toPredict], convert(Vector{Float64}, MLBase.predict(model, test)), .5))\n",
    "            end\n",
    "            \n",
    "            f_scores[i, j.RUS[1]] = f\n",
    "            \n",
    "        end\n",
    "        \n",
    "        println(k, mean(f_scores[i,:]))\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        \n",
    "    end\n",
    "    \n",
    "    return sort(DataFrame(Variables = collect(combinations), F1 = [mean(f_scores[i,:]) for i in 1:(2^length(varNames)-1)]), :F1, rev = true)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchLogisticROS(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, n::Int)\n",
    "Returns the F1 score of all possible logistic regression models in decreasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `n::Int` : The number of Randomly Undersampled datasets to use\n",
    "\n",
    "#Implementation\n",
    "The mean F1 score is computed from the individual F1 score on the test data of the models trained on \n",
    "100 randomly undersampled datasets.\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on average on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLogisticROS(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, n::Int)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    f_scores = Array{Float64}(undef, 2^length(varNames)-1, n)\n",
    "    \n",
    "    first = DataFrame(DataTable(oversample(row->row[Symbol(toPredict)], DataTable(train))))\n",
    "    l_first = size(first)[1]\n",
    "    rus = hcat(first, DataFrame(RUS = Int.(ones(l_first))))\n",
    "    \n",
    "    for i in 2:n\n",
    "        rus = vcat(rus, hcat(DataFrame(DataTable(oversample(row->row[Symbol(toPredict)], DataTable(train)))), DataFrame(RUS = Int.(i .* ones(l_first)))))\n",
    "    end\n",
    "    \n",
    "    rus_j = groupby(rus, :RUS)\n",
    "    \n",
    "    i = 1\n",
    "    for k in combinations\n",
    "        \n",
    "        \n",
    "        for j in rus_j\n",
    "            \n",
    "            local model\n",
    "            \n",
    "             \n",
    "            model = getLogisticModel(DataFrame(j), toPredict, k)\n",
    "            if model == 0.0\n",
    "                println(\"Error in model fitting\")\n",
    "                f = 0\n",
    "            else \n",
    "                f = f1score(roc(test[!,toPredict], convert(Vector{Float64}, MLBase.predict(model, test)), .5))\n",
    "            end\n",
    "            \n",
    "            f_scores[i, j.RUS[1]] = f\n",
    "            \n",
    "        end\n",
    "        \n",
    "        println(k, mean(f_scores[i,:]))\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        \n",
    "    end\n",
    "    \n",
    "    return sort(DataFrame(Variables = collect(combinations), F1 = [mean(f_scores[i,:]) for i in 1:(2^length(varNames)-1)]), :F1, rev = true)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62bfc0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: DataFrame not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: DataFrame not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[5]:20"
     ]
    }
   ],
   "source": [
    "using PyCall\n",
    "\n",
    "\"\"\"\n",
    "    extensiveSearchLogisticSMOTE(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, n::Int)\n",
    "Returns the F1 score of all possible logistic regression models in decreasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `n::Int` : The number of Randomly Undersampled datasets to use\n",
    "\n",
    "#Implementation\n",
    "The mean F1 score is computed from the individual F1 score on the test data of the models trained on \n",
    "100 randomly undersampled datasets.\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on average on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLogisticSMOTE(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, n::Int)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    f_scores = Array{Float64}(undef, 2^length(varNames)-1, n)\n",
    "    \n",
    "    imb = pyimport(\"imblearn\") \n",
    "    smt = imb.over_sampling.SMOTE(sampling_strategy = 1.0)\n",
    "    \n",
    "    d_first, first_CSO = smt.fit_resample(Array(select(train, Not([:Date, :Site, :CSO]))), train.CSO) \n",
    "    col_names_full = names(select(train, Not([:Date, :Site, :CSO])))\n",
    "    d_first = DataFrame(hcat(d_first, first_CSO), vcat(col_names_full,String.([:CSO])))\n",
    "    \n",
    "    l_first = size(d_first)[1]\n",
    "    smote = hcat(d_first, DataFrame(SMOTE = Int.(ones(l_first))))\n",
    "    \n",
    "    for i in 2:n\n",
    "        \n",
    "        current, current_CSO = smt.fit_resample(Array(select(train, Not([:Date, :Site, :CSO]))), train.CSO)\n",
    "        current = DataFrame(hcat(current, current_CSO), vcat(col_names_full,String.([:CSO])))\n",
    "        smote = vcat(smote, hcat(current, DataFrame(SMOTE = Int.(i .* ones(l_first)))))\n",
    "    end\n",
    "    \n",
    "    smote_j = groupby(smote, :SMOTE)\n",
    "    \n",
    "    i = 1\n",
    "    for k in combinations\n",
    "        \n",
    "        \n",
    "        for j in smote_j\n",
    "            \n",
    "            local model\n",
    "            \n",
    "             \n",
    "            model = getLogisticModel(DataFrame(j), toPredict, k)\n",
    "            if model == 0.0\n",
    "                println(\"Error in model fitting\")\n",
    "                f = 0\n",
    "            else \n",
    "                f = f1score(roc(test[!,toPredict], convert(Vector{Float64}, MLBase.predict(model, test)), .5))\n",
    "            end\n",
    "            \n",
    "            f_scores[i, j.SMOTE[1]] = f\n",
    "            \n",
    "        end\n",
    "        \n",
    "        println(k, mean(f_scores[i,:]))\n",
    "        \n",
    "        i = i+1\n",
    "        \n",
    "        \n",
    "    end\n",
    "    \n",
    "    return sort(DataFrame(Variables = collect(combinations), F1 = [mean(f_scores[i,:]) for i in 1:(2^length(varNames)-1)]), :F1, rev = true)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8f0a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchLinear(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String)\n",
    "Returns the mean squared error of all possible linear regression models in increasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `criteria::String` : The goodness of fit indicator to be used.\n",
    "\n",
    "#Implementation\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchLinear(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    \n",
    "    if (criteria == \"RMSD_CSO\" )|| (criteria == \"MSE_CSO\")\n",
    "        l = 1\n",
    "        for i in combinations\n",
    "            model = getLinearModel(train[train[!,toPredict] .> 0,:], toPredict, i)\n",
    "            mse = compute_MSE(model, test[test[!,toPredict] .> 0, :], toPredict)\n",
    "            scores[l] = mse\n",
    "            l = l+1\n",
    "        end\n",
    "        \n",
    "        if criteria == \"MSE\"\n",
    "            return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "        elseif criteria == \"RMSD\"\n",
    "            return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        model = getLinearModel(train, toPredict, i)\n",
    "        mse = compute_MSE(model, test, toPredict)\n",
    "        scores[l] = mse\n",
    "        \n",
    "        println(i, scores[l])\n",
    "        l = l+1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc0b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extensiveSearchGamma(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String, link::String)\n",
    "Returns the mean squared error or rmsd of all possible Gamma regression models in increasing order.\n",
    "\n",
    "# Arguments\n",
    "- `train::DataFrame`: The datafram containing the training data for the model.\n",
    "- `test::DataFrame`: The datafram containing the test data for the model.\n",
    "- `toPredict::String`:The name of the column of `df` containing the variable of interest to be predicted.\n",
    "- `predictors::Vector{String}` : The names of the column of `df` containing the predictors.\n",
    "- `criteria::String` : The goodness of fit indicator to be used.\n",
    "- `link::String` : The link function to be used.\n",
    "\n",
    "#Implementation\n",
    "The data is returned in the form of a 2 column matrix with the best performing models on top.\n",
    "\"\"\"\n",
    "\n",
    "function extensiveSearchGamma(train::DataFrame, test::DataFrame, toPredict::String, varNames::Vector{String}, criteria::String, link::String)\n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "        \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        model = getGammaModel(train, toPredict, i, link)\n",
    "\n",
    "        mse = compute_MSE(model, test, toPredict)\n",
    "        scores[l] = mse\n",
    "        \n",
    "        println(i, mse)\n",
    "        l = l+1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions, SpecialFunctions\n",
    "\n",
    "function log_likelihood_bc(x, lambda)\n",
    "    n = length(x)\n",
    "    if lambda == 0\n",
    "        y = log.(x)\n",
    "    else \n",
    "        y = (x .^lambda .-1)/lambda\n",
    "    end\n",
    "    \n",
    "    sigma_sq = var(y, corrected = false)\n",
    "    mu = mean(y)\n",
    "    \n",
    "    return -n/2*log(sigma_sq) - 1/(2*sigma_sq)*(y .-mu)'*(y .- mu) + (lambda-1)*sum(log.(x))\n",
    "end\n",
    "\n",
    "function log_likelihood_gamma_bc(x, lambda)\n",
    "    n = length(x)\n",
    "    if lambda == 0\n",
    "        y = log.(x)\n",
    "    else \n",
    "        y = (x .^lambda .-1)/lambda\n",
    "    end\n",
    "    \n",
    "    sigma_sq = var(y, corrected = false)\n",
    "    mu = mean(y)\n",
    "    \n",
    "    beta = mu/sigma_sq\n",
    "    alpha = mu^2/sigma_sq\n",
    "    \n",
    "    return alpha*n*log(beta) -n*log(gamma(alpha)) + (alpha-1)*sum(log.(y)) - beta*sum(y) + (lambda-1)*sum(log.(x))\n",
    "end\n",
    "\n",
    "\n",
    "function getCI(log_likelihood, lambda_0, x)\n",
    "    \n",
    "    low = optimize(lambda -> abs(2*(log_likelihood(x, lambda_0) - log_likelihood(x, lambda))-cquantile(Chisq(1), .05)), lambda_0-5, lambda_0)\n",
    "    upp = optimize(lambda -> abs(2*(log_likelihood(x, lambda_0) - log_likelihood(x, lambda))-cquantile(Chisq(1), .05)), lambda_0, lambda_0+5)\n",
    "    \n",
    "    return (low.minimizer, lambda_0, upp.minimizer)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96be0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "function transf_lin_mod(data,test_year,lambda, toPredict, varNames, criteria)\n",
    "    \n",
    "    train = filter(row -> year(row.Date) != test_year, data)\n",
    "    test = filter(row -> year(row.Date) == test_year, data)\n",
    "    \n",
    "    \n",
    "    if lambda != 0\n",
    "        train[!, :new] = train.Duration .^(lambda)\n",
    "    else \n",
    "        train[!, :new] = log.(train.Duration)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    \n",
    "    \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "         model = getLinearModel(train, \"new\", i)\n",
    "            \n",
    "        if lambda != 0\n",
    "            preds = predict(model, test)\n",
    "            MSE_test = sum( (real.((complex.(preds) ).^(1/lambda)) - test.Duration).^2 ) / (nrow(test) )\n",
    "            #MSE_train = sum( (predict(model, train) .^(1/lambda) - train.new .^(1/lambda)).^2 ) / (nrow(train) -length(β̂))\n",
    "            #MSE_full = sum( (predict(model, data) .^(1/lambda) - data.new .^(1/lambda)).^2 ) / (nrow(data) -length(β̂))\n",
    "        else\n",
    "            MSE_test = sum( (exp.(predict(model, test)) - test.Duration).^2 ) / (nrow(test) )\n",
    "            #MSE_train = sum( (exp.(predict(model, train)) - exp.(train.new)).^2 ) / (nrow(train) -length(β̂))\n",
    "            #MSE_full = sum( (exp.(predict(mdoel, data)) - exp.(data.new)).^2 ) / (nrow(data) -length(β̂))\n",
    "        end\n",
    "        mse = MSE_test\n",
    "        \n",
    "        scores[l] = real(mse)\n",
    "        println(i, mse)\n",
    "        l = l+1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918faea",
   "metadata": {},
   "outputs": [],
   "source": [
    "function transf_lin_mod_LONG_DUR(data,test_year, lambda, seuil, toPredict, varNames, criteria)\n",
    "    \n",
    "    if lambda != 0\n",
    "        data[!, :new] = data.Duration .^(lambda)\n",
    "    else \n",
    "        data[!, :new] = log.(data.Duration)\n",
    "    end\n",
    "    \n",
    "    train = filter(row -> year(row.Date) != test_year, data)\n",
    "    train = filter(row -> row.Duration .>= seuil, train)\n",
    "    test = filter(row -> year(row.Date) == test_year, data)\n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    \n",
    "    \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "         model = getLinearModel(train, \"new\", i)\n",
    "            \n",
    "        if lambda != 0\n",
    "            MSE_test = sum( ((complex.(predict(model, test)) ).^(1/lambda) - test.Duration).^2 ) / (nrow(test) )\n",
    "            #MSE_train = sum( (predict(model, train) .^(1/lambda) - train.new .^(1/lambda)).^2 ) / (nrow(train) -length(β̂))\n",
    "            #MSE_full = sum( (predict(model, data) .^(1/lambda) - data.new .^(1/lambda)).^2 ) / (nrow(data) -length(β̂))\n",
    "        else\n",
    "            MSE_test = sum( (exp.(predict(model, test)) - test.Duration).^2 ) / (nrow(test) )\n",
    "            #MSE_train = sum( (exp.(predict(model, train)) - exp.(train.new)).^2 ) / (nrow(train) -length(β̂))\n",
    "            #MSE_full = sum( (exp.(predict(mdoel, data)) - exp.(data.new)).^2 ) / (nrow(data) -length(β̂))\n",
    "        end\n",
    "        mse = MSE_test\n",
    "        \n",
    "        scores[l] = real(mse)\n",
    "        println(i, mse)\n",
    "        l = l+1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988b646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function transf_gamma_mod(data,test_year, lambda, toPredict, varNames, criteria)\n",
    "    \n",
    "    train = filter(row -> year(row.Date) != test_year, data)\n",
    "    test = filter(row -> year(row.Date) == test_year, data)\n",
    "    \n",
    "    if lambda != 0\n",
    "        train[!, :new] = train.Duration .^(lambda)\n",
    "    else \n",
    "        train[!, :new] = log.(train.Duration)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    \n",
    "    combinations = Combinatorics.combinations(varNames)\n",
    "    scores = Vector{Float64}(undef, 2^length(varNames)-1)\n",
    "    \n",
    "    \n",
    "    l = 1\n",
    "    for i in combinations\n",
    "        local model\n",
    "        \n",
    "        try \n",
    "            model = getGammaModel(train, \"new\", i, \"log\")\n",
    "        catch e\n",
    "            MSE_test = 10000000000\n",
    "        else\n",
    "            if lambda != 0\n",
    "                MSE_test = sum( (predict(model, test) .^(1/lambda) - test.Duration).^2 ) / (nrow(test))\n",
    "            else\n",
    "                MSE_test = sum( (exp.(predict(model, test)) - test.Duration).^2 ) / (nrow(test))\n",
    "            end\n",
    "        end\n",
    "            \n",
    "        \n",
    "        mse = MSE_test\n",
    "        \n",
    "        scores[l] = mse\n",
    "        println(i, mse)\n",
    "        l = l+1\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if criteria == \"MSE\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), MSE = scores), :MSE)\n",
    "        \n",
    "    elseif criteria == \"RMSD\"\n",
    "        return sort(DataFrame(Variables = collect(combinations), RMSD = sqrt.(scores)), :RMSD)\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080f831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_principal_components (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cumulative_variance_plot(sigma::Vector{Float64}, plot::Bool)\n",
    "    cumvar = cumsum(sigma.^2)\n",
    "\n",
    "    ratio = cumvar / cumvar[end]\n",
    "\n",
    "    var = DataFrame(k = Int64[], Variance = Float64[])\n",
    "\n",
    "    for k in 1:length(ratio)\n",
    "        push!(var, [k, ratio[k]])\n",
    "    end\n",
    "    \n",
    "    if plot\n",
    "        return plot(var, x=:k, y=:Variance, Geom.line)\n",
    "    else\n",
    "        return var\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "\n",
    "function PCA_principal_components(SVD, k)\n",
    "    \n",
    "    return SVD.U[:,1:k]*Diagonal(SVD.S[1:k])\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187efb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
